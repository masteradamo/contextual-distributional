# contextual-distributional
Code for Building Dynamically Contextual Distributional Semantic Models

This is code for building dynamically contextual distributional semantic models.  The script "modeller" takes as input a path to a text file which should contain a corpus in which every line is a sentence.  This script doesn't perform any data cleaning, so the corpus should be formatted to contain words as you would like them to be represented in your model, including the removing of punctuation, de-capitalisation, and so forth.  For instance, the strings "lion", "Lion", and "lion," would all be interpreted as different words.  Blank spaces are interpreted as word boundaries, and new lines are interpreted as sentence boundaries.

You should manually enter five arguments describing the path to your cleaned corpus, the path to which you would like the base model to be written, and three model parameters.  These arguments are as follows:

 - fold: The path to your corpus.  If the corpus is in the same folder as your script, it can just be the filename of the corpus.

 - fr: The path to the location in which you would like the base model to be written.

 - win: The co-occurrence window you would like to use to capture statistics about observations of words in your corpus.  This delimits the number of words on either side of any given target word which will be considered to co-occur with that target word.  If you would like to treat entire sentences as co-occurring bags-of-words, simply set this value to be very high.

 - lim: The size of your vocabulary.  The top most frequently occurring word types up to this value will be considered the vocabulary; all words, however, are considered potential co-occurrence words.

 - smth: The smoothing constant that will be added to the co-occurrence frequency when calculating the pointwise mutual information value for a co-occurrence.  As a rule of thumb, a value close to the median frequency of your vocabulary words across the corpus works well for many modelling objectives.  For more details on the co-occurrence statistic weighting scheme, see McGregor et al (2015), "From Distributional Semantic to Conceptual Spaces: A Novel Computational Method for Concept Creation".

The script will create a new folder in the folder that you indicate with the "fr" parameter named "[win]x[win]", where [win] is the value you indicate for the "win" parameter.  This folder will contain two further folders, one titled "vectors" and one titled "dimensions".  The script will also place a file titled "wordus.txt" in this folder, containing a list of all words observed in your corpus ranked by frequency, as well as a file titled "contextus.txt" that contains a list of the same words with the number of times they're observed as co-occurrence terms.

The script proceeds by building these lists and then iterating through the vocabulary based on the tranches described by the "cyc" parameter.  *PLEASE NOTE*, these values have been optimised for a computer with 16GB of RAM building a 5x5 word co-occurrence window model with a 200,000 word vocabulary from a corpus with approximately 1.5 billion word tokens, 7.8 million word types, and 67 million sentences.  If your corpus is smaller, or your system is more powerful, you don't need to change these values, though you should set the upper boundary of the cyc tranches to your vocabulary size.  If your corpus is bigger or your system has less RAM, though, or if you wish to use a larger co-occurrence window, please make sure to adjust these settings or you might crash your computer.

The script creates a separate file for each word in the vocabulary in the "vectors" folder, and each of these files represents a single word-vector with values for all its non-zero co-occurrence elements.  The matrix implicit in this representation scheme is then transposed into the "dimensions" folder, so that each file in that folder represents the values along a single co-occurrence dimension.  As such, the number of files in the "vectors" folder will be equal to the number of words in the vocabulary, and the number of files in the "dimensions" folder will be potentially much greater, probably close to or equal to the total number of word types in the corpus.

This is a space-hungry data structure, effectively representing the same base space twice over, but it allows for very efficient access to the data in the course of building subspaces.  The script "prober" allows a basic exploration of the base model.  If you set the variable "fold" to the folder in which the model was built (so "your/path/[win]x[win]/"), then run this script, you can enter a set of words, a dimensionality for subspaces, and a number of vectors in order to get that many vectors based on two different subspace exploration techniques.  The first technique returns the word-vectors with the highest norms up to the limit you indicate.  The second technique returns the word-vectors that are closet to the midpoint between the input word-vectors normalised to be of the average length of the input word-vectors.  Three different types of subspace are explored: "simple" subspaces where the dimensions with the highest mean value across all input word-vectors are selected, "joint" subspaces where the dimensions with the highest mean value are selected with the additional constraint that the values must be non-zero for all input word-vectors, and "indy" subspaces where an amalgamation of dimensions with the highest values for each word-vectors independently are selected. 
